{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618b6dec",
   "metadata": {},
   "source": [
    "Установите необходимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch spacy tqdm\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2cce7",
   "metadata": {},
   "source": [
    "Импортируем библиотеки и проверяем наличие GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272401e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import spacy\n",
    "import ru_core_news_sm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import unicodedata\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d3b71",
   "metadata": {},
   "source": [
    "Задаем константы. Для вас важнее всего здесь \n",
    "* N_EPOCHS - число эпох для обучения модели\n",
    "* EMB_DIM - размер эмбеддингов, в какой размер \"сжимается\" весь словарь из обучающих текстов\n",
    "* HID_DIM - размерность скрытого состояния, от этого параметра зависит \"память\" модели, как много она может запомнить\n",
    "* LEARNING_RATE - скорость обучения\n",
    "* TEACHER_FORCING_RATIO - вероятность, с которой даем модели при обучении правильные продолжения текстов, без этого модель хуже учится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad854003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Устройство для вычислений: cuda\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = \"prompt_expander_train.jsonl\"\n",
    "MODEL_PATH = \"seq2seq_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "CLIP = 1\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "try:\n",
    "    nlp = ru_core_news_sm.load(disable=[\"parser\", \"ner\"])\n",
    "except Exception as e:\n",
    "    print(f\"Не удалось загрузить модель 'ru_core_news_sm'. Убедитесь, что она установлена:\")\n",
    "    print(\"python -m spacy download ru_core_news_sm\")\n",
    "    nlp = lambda text: [tok.text for tok in spacy.blank(\"ru\").tokenizer(text)]\n",
    "print(f\"Устройство для вычислений: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8ed85",
   "metadata": {},
   "source": [
    "Функции загрузки текстов обучения и их парсинга (разбор текстов в структурированный формат)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fff8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка и парсинг данных из prompt_expander_train.jsonl...\n",
      "Загружено 621 пар промпт-ответ.\n",
      "\n",
      "Пример пары:\n",
      "ВХОД: как открыть вклад\n",
      "ВЫХОД: открыть рублевыи вклад накопительныи на месяцев с возможностью ежемесячнои капитализации процентов\n"
     ]
    }
   ],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^а-яА-Яa-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "def load_and_parse_data(filepath):\n",
    "    pairs = []\n",
    "    pattern = re.compile(r\"<s>\\[INST\\] (.*?) \\[/INST\\] (.*?)</s>\", re.DOTALL)\n",
    "    print(f\"Загрузка и парсинг данных из {filepath}...\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                text = data.get(\"text\", \"\")\n",
    "                match = pattern.search(text)\n",
    "                if match:\n",
    "                    src = normalize_string(match.group(1))\n",
    "                    trg = normalize_string(match.group(2))\n",
    "                    if src and trg:\n",
    "                        pairs.append((src, trg))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Ошибка декодирования JSON в строке: {line}\")\n",
    "                continue\n",
    "    print(f\"Загружено {len(pairs)} пар промпт-ответ.\")\n",
    "    return pairs\n",
    "try:\n",
    "    test_pairs = load_and_parse_data(DATA_FILE)\n",
    "    print(\"\\nПример пары:\")\n",
    "    print(f\"ВХОД: {test_pairs[0][0]}\")\n",
    "    print(f\"ВЫХОД: {test_pairs[0][1]}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nФайл {DATA_FILE} пока не найден. Будет загружен при обучении.\")\n",
    "except IndexError:\n",
    "    print(\"\\nФайл данных пуст или не содержит корректных пар.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a9236",
   "metadata": {},
   "source": [
    "Формирование общих словарей и токенизация (разбиение на отдельные элементы текста + сопоставление им уникальных чисел, которые подаются в модель вместо самих слов/букв и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b5f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ru(text):\n",
    "    return [token.text for token in nlp(text)]\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
    "        self.n_words = 4\n",
    "    def add_sentence(self, sentence, tokenizer):\n",
    "        for word in tokenizer(sentence):\n",
    "            self.add_word(word)\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "def build_vocabs(pairs, tokenizer):\n",
    "    print(\"Построение словарей...\")\n",
    "    input_vocab = Vocab(\"input\")\n",
    "    output_vocab = Vocab(\"output\")\n",
    "    for src, trg in tqdm(pairs):\n",
    "        input_vocab.add_sentence(src, tokenizer)\n",
    "        output_vocab.add_sentence(trg, tokenizer)\n",
    "    print(f\"Словарь Входа: {input_vocab.n_words} слов\")\n",
    "    print(f\"Словарь Выхода: {output_vocab.n_words} слов\")\n",
    "    return input_vocab, output_vocab\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "UNK_IDX = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f04e8",
   "metadata": {},
   "source": [
    "Функции загрузки датасета текстов с диска и в ОЗУ по батчам (порциям для ускорения обучения)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c24e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "def text_to_indices(text, vocab, tokenizer):\n",
    "    tokens = tokenizer(text)\n",
    "    return [vocab.word2index.get(word, UNK_IDX) for word in tokens]\n",
    "def collate_fn(batch, input_vocab, output_vocab, tokenizer, device):\n",
    "    src_list, trg_list = [], []\n",
    "    src_len_list, trg_len_list = [], []\n",
    "    for src_text, trg_text in batch:\n",
    "        src_indices = [SOS_IDX] + text_to_indices(src_text, input_vocab, tokenizer) + [EOS_IDX]\n",
    "        trg_indices = [SOS_IDX] + text_to_indices(trg_text, output_vocab, tokenizer) + [EOS_IDX]\n",
    "        src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
    "        trg_tensor = torch.tensor(trg_indices, dtype=torch.long)\n",
    "        src_list.append(src_tensor)\n",
    "        trg_list.append(trg_tensor)\n",
    "        src_len_list.append(len(src_indices))\n",
    "        trg_len_list.append(len(trg_indices))\n",
    "    src_padded = pad_sequence(src_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    trg_padded = pad_sequence(trg_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    src_lengths = torch.tensor(src_len_list, dtype=torch.long)\n",
    "    trg_lengths = torch.tensor(trg_len_list, dtype=torch.long)\n",
    "    return (\n",
    "        src_padded.to(device),\n",
    "        src_lengths.to(\"cpu\"),\n",
    "        trg_padded.to(device),\n",
    "        trg_lengths.to(\"cpu\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9350d",
   "metadata": {},
   "source": [
    "Seq2Seq состоит из частей - энкодера и декодера, а также использует слой внимания. Задаем энкодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe954912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim,\n",
    "            hid_dim,\n",
    "            n_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, src_len):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_len, batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.rnn(packed_embedded)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        hidden = hidden.contiguous().view(-1, self.n_layers, self.hid_dim * 2)\n",
    "        hidden = torch.tanh(self.fc(hidden)).permute(1, 0, 2).contiguous()\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675621f",
   "metadata": {},
   "source": [
    "Теперь слой внимания (который учитывает взаимосвязь между словами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b47992fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn_W = nn.Linear(hid_dim * 2 + hid_dim, hid_dim)\n",
    "        self.attn_v = nn.Linear(hid_dim, 1, bias=False)\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden_repeated = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy_input = torch.cat((hidden_repeated, encoder_outputs), dim=2)\n",
    "        energy = torch.tanh(self.attn_W(energy_input))\n",
    "        attention_scores = self.attn_v(energy)\n",
    "        attention_scores = attention_scores.squeeze(2)\n",
    "        return F.softmax(attention_scores, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562088c7",
   "metadata": {},
   "source": [
    "И декодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03acfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(\n",
    "            (hid_dim * 2) + emb_dim,\n",
    "            hid_dim,\n",
    "            n_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(\n",
    "            emb_dim + hid_dim + (hid_dim * 2),\n",
    "            output_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        input_token = input_token.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        attn_weights_unsqueezed = attn_weights.unsqueeze(1)\n",
    "        context_vector = torch.bmm(attn_weights_unsqueezed, encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context_vector), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        embedded_s = embedded.squeeze(1)\n",
    "        output_s = output.squeeze(1)\n",
    "        context_s = context_vector.squeeze(1)\n",
    "        prediction_input = torch.cat((embedded_s, output_s, context_s), dim=1)\n",
    "        prediction = self.fc_out(prediction_input)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12343bb",
   "metadata": {},
   "source": [
    "Остается собрать модель воедино"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9a4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        input_token = trg[:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input_token, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = trg[:, t] if use_teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6b568",
   "metadata": {},
   "source": [
    "Цикл обучения с расчетом времени и тп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed6811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Обучение\", leave=False)\n",
    "    for i, (src, src_len, trg, trg_len) in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_len, trg, TEACHER_FORCING_RATIO)\n",
    "        output_dim = output.shape[-1]\n",
    "        output_flat = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg_flat = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(output_flat, trg_flat)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.3f}\")\n",
    "    return epoch_loss / len(dataloader)\n",
    "def format_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fc511",
   "metadata": {},
   "source": [
    "Данная ячейка запускает обучение модели, выводятся данные по эпохам. PPL здесь - мера \"запутанности\" модели, сколько кандидатов-токенов она видит для конкретного одного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19180480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найден сохраненный файл модели: seq2seq_model.pth\n",
      "Загрузка модели и словарей...\n",
      "Словари загружены.\n",
      "Параметры модели: {'INPUT_DIM': 887, 'OUTPUT_DIM': 2766, 'EMB_DIM': 256, 'HID_DIM': 512, 'N_LAYERS': 2, 'ENC_DROPOUT': 0.5, 'DEC_DROPOUT': 0.5}\n",
      "Модель успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "input_vocab = None\n",
    "output_vocab = None\n",
    "params = {}\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"Найден сохраненный файл модели: {MODEL_PATH}\")\n",
    "    print(\"Загрузка модели и словарей...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "        input_vocab = checkpoint['input_vocab']\n",
    "        output_vocab = checkpoint['output_vocab']\n",
    "        params = checkpoint['params']\n",
    "        print(\"Словари загружены.\")\n",
    "        print(f\"Параметры модели: {params}\")\n",
    "        attn = Attention(params['HID_DIM'])\n",
    "        enc = Encoder(\n",
    "            input_vocab.n_words,\n",
    "            params['EMB_DIM'],\n",
    "            params['HID_DIM'],\n",
    "            params['N_LAYERS'],\n",
    "            params['ENC_DROPOUT']\n",
    "        )\n",
    "        dec = Decoder(\n",
    "            output_vocab.n_words,\n",
    "            params['EMB_DIM'],\n",
    "            params['HID_DIM'],\n",
    "            params['N_LAYERS'],\n",
    "            params['DEC_DROPOUT'],\n",
    "            attn\n",
    "        )\n",
    "        model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(\"Модель успешно загружена.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке модели: {e}. Начинаем обучение с нуля.\")\n",
    "        model = None\n",
    "else:\n",
    "    print(f\"Файл модели {MODEL_PATH} не найден. Начинаем обучение...\")\n",
    "if model is None:\n",
    "    all_pairs = load_and_parse_data(DATA_FILE)\n",
    "    random.shuffle(all_pairs)\n",
    "    train_pairs = all_pairs\n",
    "    input_vocab, output_vocab = build_vocabs(train_pairs, tokenize_ru)\n",
    "    train_dataset = PromptDataset(train_pairs)\n",
    "    collate_with_vocabs = lambda batch: collate_fn(\n",
    "        batch, input_vocab, output_vocab, tokenize_ru, DEVICE\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_with_vocabs\n",
    "    )\n",
    "    print(\"DataLoader'ы созданы.\")\n",
    "    params = {\n",
    "        'INPUT_DIM': input_vocab.n_words,\n",
    "        'OUTPUT_DIM': output_vocab.n_words,\n",
    "        'EMB_DIM': EMB_DIM,\n",
    "        'HID_DIM': HID_DIM,\n",
    "        'N_LAYERS': N_LAYERS,\n",
    "        'ENC_DROPOUT': ENC_DROPOUT,\n",
    "        'DEC_DROPOUT': DEC_DROPOUT\n",
    "    }\n",
    "    attn = Attention(HID_DIM)\n",
    "    enc = Encoder(\n",
    "        params['INPUT_DIM'], EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT\n",
    "    )\n",
    "    dec = Decoder(\n",
    "        params['OUTPUT_DIM'], EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn\n",
    "    )\n",
    "    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "    print(f\"Модель инициализирована и перемещена на {DEVICE}.\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = format_time(start_time, end_time)\n",
    "        print(f'Эпоха: {epoch+1:02} | Время: {epoch_mins}м {epoch_secs}с')\n",
    "        print(f'\\tLoss Обучения: {train_loss:.3f} | PPL: {math.exp(train_loss):7.3f}')\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            print(\"Новая лучшая модель. Сохранение...\")\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'input_vocab': input_vocab,\n",
    "                'output_vocab': output_vocab,\n",
    "                'params': params,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, MODEL_PATH)\n",
    "    print(\"Обучение завершено.\")\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafe4a4",
   "metadata": {},
   "source": [
    "Код функций для инференса, сам инференс в следующей ячейке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f483852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "def predict_expansion(\n",
    "    sentence,\n",
    "    model,\n",
    "    input_vocab,\n",
    "    output_vocab,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_len=250\n",
    "):\n",
    "    model.eval()\n",
    "    normalized_sentence = normalize_string(sentence)\n",
    "    tokens = [SOS_TOKEN] + tokenizer(normalized_sentence) + [EOS_TOKEN]\n",
    "    indices = [input_vocab.word2index.get(token, UNK_IDX) for token in tokens]\n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "    src_len = torch.LongTensor([len(indices)]).to(\"cpu\")\n",
    "    decoded_words = []\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "        trg_input = torch.LongTensor([SOS_IDX]).to(device)\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = model.decoder(trg_input, hidden, encoder_outputs)\n",
    "            topv, topi = output.topk(1)\n",
    "            if topi.item() == EOS_IDX:\n",
    "                break\n",
    "            word = output_vocab.index2word.get(topi.item(), UNK_TOKEN)\n",
    "            decoded_words.append(word)\n",
    "            trg_input = topi.squeeze(1).detach()\n",
    "    return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4086817",
   "metadata": {},
   "source": [
    "Запускаете в подобном ollama режиме - появляется текстовое окно для ввода запроса (краткаий промпт) - после ввода и нажатия Enter получаете ответ (развернутый промпт), появляется новое поле ввода до тех пор, пока не введете exit или выход и не подтвердите Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71e21f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Модель готова к работе.\n",
      "Введите ваш краткий запрос (например, 'Как открыть вклад')\n",
      "Для выхода введите 'выход' или 'exit'.\n",
      "==================================================\n",
      "\n",
      "[МОДЕЛЬ]:\n",
      "узнать стоимость вложения в\n",
      "\n",
      "[МОДЕЛЬ]:\n",
      "узнать стоимость вложения в\n",
      "\n",
      "[МОДЕЛЬ]:\n",
      "наити в личном кабинете\n",
      "\n",
      "[МОДЕЛЬ]:\n",
      "проблема не может в пересчитать в праздничные дни\n",
      "Работа завершена.\n"
     ]
    }
   ],
   "source": [
    "if model is None or input_vocab is None or output_vocab is None:\n",
    "    print(\"Ошибка: Модель не была загружена или обучена.\")\n",
    "    print(\"Пожалуйста, сначала запустите Ячейку 11.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Модель готова к работе.\")\n",
    "    print(\"Введите ваш краткий запрос (например, 'Как открыть вклад')\")\n",
    "    print(\"Для выхода введите 'выход' или 'exit'.\")\n",
    "    print(\"=\"*50)\n",
    "    while True:\n",
    "        try:\n",
    "            input_prompt = input(\"\\n[ВЫ]: \")\n",
    "            if input_prompt.lower() in ['выход', 'exit', 'quit']:\n",
    "                print(\"Работа завершена.\")\n",
    "                break\n",
    "            if not input_prompt:\n",
    "                continue\n",
    "            predicted_text = predict_expansion(\n",
    "                input_prompt,\n",
    "                model,\n",
    "                input_vocab,\n",
    "                output_vocab,\n",
    "                tokenize_ru,\n",
    "                DEVICE\n",
    "            )\n",
    "            print(\"\\n[МОДЕЛЬ]:\")\n",
    "            lines = predicted_text.split('\\n')\n",
    "            for line in lines:\n",
    "                print('\\n'.join(wrap(line, width=100)))\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nРабота прервана. Выход...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nПроизошла ошибка: {e}\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
