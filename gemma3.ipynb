{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, default_data_collator\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#грузим файл\n",
    "file_path = 'train.jsonl'\n",
    "# качаем с huggingface модель\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    " \n",
    "# конфигурация для 4-битного квантования (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "\tload_in_4bit=True,\n",
    "\tbnb_4bit_quant_type=\"nf4\",\n",
    "\tbnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    " \n",
    "# модель с квантованием\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tmodel_id,\n",
    "\tquantization_config=bnb_config,\n",
    "\t# если есть GPU nvidia на него модель\n",
    "    device_map={\"\": \"cuda:0\"} if torch.cuda.is_available() else {\"\": \"cpu\"}\n",
    ")\n",
    " \n",
    "# токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# токен для паддинга - это важный шаг\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Дополнительно, установите токен паддинга как токен конца предложения,\n",
    "# чтобы избежать предупреждений.\n",
    "tokenizer.padding_side = \"right\" # Хорошая практика для CausalLM\n",
    " \n",
    " \n",
    "# создаем датасет из jsonl файла\n",
    "ds = load_dataset(\"json\", data_files=\"train.jsonl\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "\toutput_dir=\"./gemma-finetuned\",\n",
    "\tper_device_train_batch_size=1,\n",
    "\tgradient_accumulation_steps=4,\n",
    "\tlearning_rate=2e-4,\n",
    "\tnum_train_epochs=3,\n",
    "\tlogging_steps=10,\n",
    "\tsave_strategy=\"epoch\",\n",
    "\tfp16=True if torch.cuda.is_available() else False, # fp16 работает только на CUDA\n",
    ")\n",
    " \n",
    "# LoRA конфиг\n",
    "lora_config = LoraConfig(\n",
    "\tr=8,\n",
    "\ttarget_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "\ttask_type=\"CAUSAL_LM\",\n",
    ")\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "\tmodel=model,\n",
    "\targs=training_args,\n",
    "\t# передаем сырой датасет.\n",
    "\ttrain_dataset=ds,\n",
    "\tpeft_config=lora_config,\n",
    "\t# передаем токенизатор\n",
    "\tprocessing_class=tokenizer,\n",
    ")\n",
    " \n",
    "# запускаем файн-тюнинг\n",
    "trainer.train()\n",
    " \n",
    "print(\"Обучение успешно завершено!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1266560",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"google/gemma-3-1b-it\"\n",
    "output_dir = \"./gemma-finetuned\"\n",
    "merged_model_dir = \"./gemma-finetuned-merged\"\n",
    "all_checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint-*'))\n",
    "if not all_checkpoints:\n",
    "\traise FileNotFoundError(f\"Не найдены папки 'checkpoint-' в каталоге {output_dir}. Убедитесь, что обучение завершилось.\")\n",
    "def extract_step(checkpoint_path):\n",
    "\tmatch = re.search(r'checkpoint-(\\d+)', checkpoint_path)\n",
    "\treturn int(match.group(1)) if match else -1\n",
    "latest_checkpoint = max(all_checkpoints, key=extract_step)\n",
    "lora_adapter_dir = latest_checkpoint\n",
    "print(f\"последний чекпоинт: {lora_adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Загрузка базовой модели...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "\tbase_model_id,\n",
    "\ttorch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "\tdevice_map={\"\": \"cuda:0\"} if torch.cuda.is_available() else {\"\": \"cpu\"}\n",
    ")\n",
    "# загрузка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "# слияние адаптера LoRA\n",
    "print(\"Слияние LoRA-адаптера с базовой моделью...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_adapter_dir)\n",
    "merged_model = model.merge_and_unload()\n",
    "# сохраняем объединенную модель в папку\n",
    "os.makedirs(merged_model_dir, exist_ok=True)\n",
    "merged_model.save_pretrained(merged_model_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "print('Готово')\n",
    "print(f\"Файлы модели находятся в: {merged_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# клонируем репозиторий llama.cpp для работы с моделями (здесь для компиляции)\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "# установим зависимости - по идее не стоит, ставит слишком много всякого,\n",
    "# установок в начале работы должно хватить\n",
    "# REQURIMENTS_LLAMA = str(Path.cwd()) + \"\\\\llama.cpp\\\\requirements.txt\"\n",
    "# !pip install -r \"{REQURIMENTS_LLAMA}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# указываем пути до модели, выходного  скрипта конвертации\n",
    "merged_model_dir = Path.cwd() / Path(\"./gemma-finetuned-merged\")\n",
    "HF_MODEL_PATH_STR = str(merged_model_dir)\n",
    "GGUF_MODEL_PATH_STR = str(merged_model_dir / \"gemma-finetuned-temp.gguf\")\n",
    "CONVERT_SCRIPT_PATH = \"./llama.cpp/convert_hf_to_gguf.py\"\n",
    " \n",
    "# запуск скрипта конвертации\n",
    "# --outtype f16 - это базовый формат при конвертации для ollama\n",
    "!python {CONVERT_SCRIPT_PATH} \"{HF_MODEL_PATH_STR}\" --outtype f16 --outfile \"{GGUF_MODEL_PATH_STR}\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
